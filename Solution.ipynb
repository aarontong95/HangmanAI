{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hangmanai.preprocess import create_input_sample\n",
    "sample = ['abc']\n",
    "features, labels = create_input_sample(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the combinations of masked word will be generated as the training samples. For example,  the word 'abc' has 7 combinations ('#bc', 'a#c', 'ab#', 'a##', '#b#', '##c', '###')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each training sample, the input length is 29 because of the maximum length of word in the trainng file.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0-25 represents A-Z\n",
    "<br>26 represents the masked value\n",
    "<br>27 represents the padding value\n",
    "<br>For example 'a#c', the input value is [2, 1, 4] + [0]*26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26,\n",
       " 1,\n",
       " 2,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 27]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label is the actual value of the letter which is masked.\n",
    "<br> Thus the length of the label is also 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-1 indicates that it will be ignored and does not contribute to the input gradient\n",
    "<br>FOR example 'a#c', the label is [-1, 1, -1...-1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Bidirectional LSTM Model is used.  \n",
    "<br>One Hot Encoding is used to ecode the input value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, isTag=True):\n",
    "        super(LSTMModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.isTag = isTag\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = Fun.one_hot(x, num_classes=self.input_dim).type(torch.FloatTensor)\n",
    "        h0 = torch.zeros(self.layer_dim*2, x.size(0), self.hidden_dim)\n",
    "        c0 = torch.zeros(self.layer_dim*2, x.size(0), self.hidden_dim)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        if not self.isTag:\n",
    "            out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output probabilities of the masked tokens are calculated. \n",
    "<br>Then Probability Score is calculated by summing up the probalilites of the masked tokens. \n",
    "<br>Finally, the Probability Score is sorted in descending order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess Letter : e, Probability Score: 0.9788843393325806\n",
      "Guess Letter : i, Probability Score: 0.5304452180862427\n",
      "Guess Letter : o, Probability Score: 0.19663839042186737\n",
      "Guess Letter : h, Probability Score: 0.11021000891923904\n",
      "Guess Letter : u, Probability Score: 0.08457174152135849\n",
      "Guess Letter : y, Probability Score: 0.05383685231208801\n",
      "Guess Letter : s, Probability Score: 0.019202565774321556\n",
      "Guess Letter : l, Probability Score: 0.008124561980366707\n",
      "Guess Letter : n, Probability Score: 0.006252141669392586\n",
      "Guess Letter : d, Probability Score: 0.004193069878965616\n",
      "Guess Letter : m, Probability Score: 0.0027985151391476393\n",
      "Guess Letter : c, Probability Score: 0.002747895661741495\n",
      "Guess Letter : f, Probability Score: 0.0008452110923826694\n",
      "Guess Letter : w, Probability Score: 0.00045734710874967277\n",
      "Guess Letter : r, Probability Score: 0.0003708497970364988\n",
      "Guess Letter : k, Probability Score: 0.00013670269981957972\n",
      "Guess Letter : p, Probability Score: 5.347983096726239e-05\n",
      "Guess Letter : v, Probability Score: 3.852080772048794e-05\n",
      "Guess Letter : b, Probability Score: 3.764823486562818e-05\n",
      "Guess Letter : j, Probability Score: 3.475197445368394e-05\n",
      "Guess Letter : z, Probability Score: 3.141316119581461e-05\n",
      "Guess Letter : t, Probability Score: 2.9344475478865206e-05\n",
      "Guess Letter : g, Probability Score: 2.6864781830227003e-05\n",
      "Guess Letter : a, Probability Score: 2.5279327019234188e-05\n",
      "Guess Letter : x, Probability Score: 3.976537755079335e-06\n",
      "Guess Letter : q, Probability Score: 3.211350076526287e-06\n"
     ]
    }
   ],
   "source": [
    "from hangmanai.model import load_model\n",
    "from hangmanai.preprocess import preprocess_feature, output_to_char\n",
    "from hangmanai.config import MAX_LEN, COVER_VALUE\n",
    "\n",
    "from config import MODEL_PATH\n",
    "model_name = 'bidiretionLSTM_0.torch'\n",
    "\n",
    "word = 'aggr_gat_'\n",
    "model = load_model(f'{MODEL_PATH}/{model_name}')\n",
    "feature = preprocess_feature(word)\n",
    "featuresTest = torch.tensor(feature).to(torch.int64)\n",
    "featuresTest = featuresTest.reshape(1, MAX_LEN)\n",
    "proba = model(featuresTest)\n",
    "proba = torch.nn.functional.softmax(proba[featuresTest==COVER_VALUE], dim=1).sum(dim=0)\n",
    "_, indices = torch.sort(proba, descending=True)\n",
    "for i in indices:\n",
    "    guess_letter = output_to_char(i)\n",
    "    probit = proba_sum[i].item()\n",
    "    print(f\"Guess Letter : {guess_letter}, Probability Score: {probit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
